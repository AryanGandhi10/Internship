{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f69faa4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in ./anaconda3/lib/python3.11/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./anaconda3/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.4)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=485f6e001ae5a499eaf3b134b945f9c2fc8d9e8c34ba9e2a498b7d737d57cfec\n",
      "  Stored in directory: /Users/aryangandhi/Library/Caches/pip/wheels/d4/c8/5b/b5be9c20e5e4503d04a6eac8a3cd5c2393505c29f02bea0960\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.11/site-packages (from requests) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "093c7e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0        India\\nIND      55  6,640    121\n",
      "1    Australia\\nAUS      42  4,926    117\n",
      "2  South Africa\\nSA      34  3,750    110\n",
      "3     Pakistan\\nPAK      36  3,922    109\n",
      "4   New Zealand\\nNZ      43  4,399    102\n",
      "5      England\\nENG      38  3,777     99\n",
      "6     Sri Lanka\\nSL      47  4,134     88\n",
      "7   Bangladesh\\nBAN      44  3,836     87\n",
      "8  Afghanistan\\nAFG      30  2,533     84\n",
      "9   West Indies\\nWI      38  2,582     68\n"
     ]
    }
   ],
   "source": [
    "#Ans3(A): Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "team_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  team = cells[1].text.strip()\n",
    "  matches = cells[2].text.strip()\n",
    "  points = cells[3].text.strip()\n",
    "  rating = cells[4].text.strip()\n",
    "  team_data.append([team, matches, points, rating])\n",
    "\n",
    "df = pd.DataFrame(team_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67910389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Batsman Team Rating\n",
      "0           Shubman Gill  IND    826\n",
      "1             Babar Azam  PAK    824\n",
      "2            Virat Kohli  IND    791\n",
      "3           Rohit Sharma  IND    769\n",
      "4        Quinton de Kock   SA    760\n",
      "5         Daryl Mitchell   NZ    750\n",
      "6           David Warner  AUS    745\n",
      "7  Rassie van der Dussen   SA    735\n",
      "8           Harry Tector  IRE    729\n",
      "9            Dawid Malan  ENG    729\n"
     ]
    }
   ],
   "source": [
    "#Ans3(B): Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsman_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  batsman = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  batsman_data.append([batsman, team, rating])\n",
    "\n",
    "df = pd.DataFrame(batsman_data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6c615d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Bowler Team Rating\n",
      "0  Keshav Maharaj   SA    741\n",
      "1  Josh Hazlewood  AUS    703\n",
      "2  Mohammed Siraj  IND    699\n",
      "3  Jasprit Bumrah  IND    685\n",
      "4      Adam Zampa  AUS    675\n",
      "5     Rashid Khan  AFG    667\n",
      "6   Kuldeep Yadav  IND    667\n",
      "7     Trent Boult   NZ    663\n",
      "8  Shaheen Afridi  PAK    650\n",
      "9  Mohammad Shami  IND    648\n"
     ]
    }
   ],
   "source": [
    "#Ans3(C): Top 10 ODI Bowlers along with the records of their team andrating.\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "bowler_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  bowler = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  bowler_data.append([bowler, team, rating])\n",
    "\n",
    "df = pd.DataFrame(bowler_data, columns=[\"Bowler\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b463b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      21  3,429    163\n",
      "1      England\\nENG      23  2,991    130\n",
      "2  South Africa\\nSA      21  2,446    116\n",
      "3        India\\nIND      18  1,745     97\n",
      "4   New Zealand\\nNZ      21  2,014     96\n",
      "5   West Indies\\nWI      20  1,768     88\n",
      "6     Sri Lanka\\nSL       9    714     79\n",
      "7   Bangladesh\\nBAN      14  1,074     77\n",
      "8     Thailand\\nTHA      11    753     68\n",
      "9     Pakistan\\nPAK      24  1,602     67\n"
     ]
    }
   ],
   "source": [
    "#Ans4(A): Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "team_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  team = cells[1].text.strip()\n",
    "  matches = cells[2].text.strip()\n",
    "  points = cells[3].text.strip()\n",
    "  rating = cells[4].text.strip()\n",
    "  team_data.append([team, matches, points, rating])\n",
    "\n",
    "df = pd.DataFrame(team_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d948959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Batsman Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    807\n",
      "1           Beth Mooney  AUS    750\n",
      "2   Chamari Athapaththu   SL    736\n",
      "3       Laura Wolvaardt   SA    727\n",
      "4       Smriti Mandhana  IND    708\n",
      "5          Alyssa Healy  AUS    698\n",
      "6          Ellyse Perry  AUS    697\n",
      "7      Harmanpreet Kaur  IND    694\n",
      "8           Meg Lanning  AUS    662\n",
      "9        Marizanne Kapp   SA    642\n"
     ]
    }
   ],
   "source": [
    "#Ans4(B): Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsman_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  batsman = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  batsman_data.append([batsman, team, rating])\n",
    "\n",
    "df = pd.DataFrame(batsman_data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "041de709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            all_rounder Team Rating\n",
      "0        Marizanne Kapp   SA    385\n",
      "1      Ashleigh Gardner  AUS    377\n",
      "2  Natalie Sciver-Brunt  ENG    360\n",
      "3       Hayley Matthews   WI    358\n",
      "4           Amelia Kerr   NZ    346\n",
      "5         Deepti Sharma  IND    312\n",
      "6          Ellyse Perry  AUS    282\n",
      "7              Nida Dar  PAK    240\n",
      "8         Jess Jonassen  AUS    227\n",
      "9         Sophie Devine   NZ    227\n"
     ]
    }
   ],
   "source": [
    "#Ans4(C): Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "all_rounder_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  all_rounder = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  all_rounder_data.append([all_rounder, team, rating])\n",
    "\n",
    "df = pd.DataFrame(all_rounder_data, columns=[\"all_rounder\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18d81360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline  Time  \\\n",
      "0   Munger and Buffett were unable to pull off one...  None   \n",
      "1   Charlie Munger said Berkshire would be worth d...  None   \n",
      "2   Stocks making the biggest moves after hours: D...  None   \n",
      "3   Stock futures mixed on Friday after Dow notche...  None   \n",
      "4   Bill Gates shares his ‘big hope’ for COP28 as ...  None   \n",
      "5   Countries at COP28 approve climate disaster fu...  None   \n",
      "6   Bill Gates: I have hope in messages coming out...  None   \n",
      "7   We'll never achieve energy transition goals wi...  None   \n",
      "8   A showdown is brewing over money, oil and carb...  None   \n",
      "9   Ukraine war live updates: Zelenskyy calls for ...  None   \n",
      "10  Multiple civilians injured in strikes on Ukrai...  None   \n",
      "11  Russia slams Finland's border closure, warns t...  None   \n",
      "12  Finland to close entire border with Russia; wi...  None   \n",
      "13  Ukraine's push into Russian-occupied land may ...  None   \n",
      "14               CNBC's Sustainable Future Forum 2023  None   \n",
      "15  Wind power industry in moment of reckoning as ...  None   \n",
      "16  CEO on why natural gas infrastructure must be ...  None   \n",
      "17  Why one Tesla manager thinks used cars are 'ab...  None   \n",
      "18  Volvo Cars CEO strikes cautious tone on solid-...  None   \n",
      "19  Southeast Asia is on the cusp of a 'supercharg...  None   \n",
      "20  Cambodia's deputy prime minister: BRI has help...  None   \n",
      "21  Southeast Asia's first luxury hotel made from ...  None   \n",
      "22  Ahead of Indonesia’s elections, critics slam J...  None   \n",
      "23  Laos is spiraling toward a debt crisis as Chin...  None   \n",
      "24  Pack your jerseys. The era of 'sports tourism'...  None   \n",
      "25  See the photos that made National Geographic's...  None   \n",
      "26  The ultimate work perk? This company provides ...  None   \n",
      "27  Fear is driving Chinese travelers away from tw...  None   \n",
      "28  A UNESCO World Heritage site with thousands of...  None   \n",
      "29  Parenting tips from a dad who’s fostered 36 ki...  None   \n",
      "30  6 tips to really disconnect from work for the ...  None   \n",
      "31  Study: Anxious dads raise smarter, more well-b...  None   \n",
      "32  On ChatGPT's one-year anniversary, it has more...  None   \n",
      "33  The No. 1 thing to avoid in a job interview, s...  None   \n",
      "\n",
      "                                            News Link  \n",
      "0   https://www.cnbc.com/2023/11/30/munger-and-buf...  \n",
      "1   https://www.cnbc.com/2023/11/30/munger-said-be...  \n",
      "2   https://www.cnbc.com/2023/11/30/stocks-making-...  \n",
      "3   https://www.cnbc.com/2023/11/30/stock-market-t...  \n",
      "4   https://www.cnbc.com/2023/12/01/bill-gates-sha...  \n",
      "5   https://www.cnbc.com/2023/11/30/cop28-delivers...  \n",
      "6   https://www.cnbc.com/video/2023/12/01/bill-gat...  \n",
      "7   https://www.cnbc.com/video/2023/11/30/we-wont-...  \n",
      "8   https://www.cnbc.com/2023/11/29/cop28-climate-...  \n",
      "9   https://www.cnbc.com/2023/12/01/russia-ukraine...  \n",
      "10  https://www.cnbc.com/2023/11/30/ukraine-war-li...  \n",
      "11  https://www.cnbc.com/2023/11/29/ukraine-war-li...  \n",
      "12  https://www.cnbc.com/2023/11/28/ukraine-war-li...  \n",
      "13  https://www.cnbc.com/2023/11/27/ukraines-bold-...  \n",
      "14  https://www.cnbc.com/2023/11/29/cnbcs-sustaina...  \n",
      "15  https://www.cnbc.com/2023/11/13/wind-power-ind...  \n",
      "16  https://www.cnbc.com/2023/11/09/ceo-on-why-nat...  \n",
      "17  https://www.cnbc.com/2023/11/02/one-tesla-mana...  \n",
      "18  https://www.cnbc.com/2023/10/27/volvo-cars-ceo...  \n",
      "19  https://www.cnbc.com/2023/11/29/southeast-asia...  \n",
      "20  https://www.cnbc.com/video/2023/11/27/cambodia...  \n",
      "21  https://www.cnbc.com/2023/11/27/a-new-luxury-h...  \n",
      "22  https://www.cnbc.com/2023/11/20/indonesia-pres...  \n",
      "23  https://www.cnbc.com/2023/11/09/laos-is-spiral...  \n",
      "24  https://www.cnbc.com/2023/11/27/pack-your-jers...  \n",
      "25  https://www.cnbc.com/2023/11/23/best-photograp...  \n",
      "26  https://www.cnbc.com/2023/11/23/remote-company...  \n",
      "27  https://www.cnbc.com/2023/11/20/fear-driving-c...  \n",
      "28  https://www.cnbc.com/2023/11/12/a-unesco-world...  \n",
      "29  https://www.cnbc.com/2023/12/01/parenting-tips...  \n",
      "30  https://www.cnbc.com/2023/11/30/6-tips-to-real...  \n",
      "31  https://www.cnbc.com/2023/11/30/study-anxious-...  \n",
      "32  https://www.cnbc.com/2023/11/30/chatgpts-one-y...  \n",
      "33  https://www.cnbc.com/2023/11/30/number-one-thi...  \n"
     ]
    }
   ],
   "source": [
    "#Ans5:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "articles = soup.find_all(\"div\", class_=\"Card-titleContainer\")\n",
    "headlines = []\n",
    "times = []\n",
    "links = []\n",
    "\n",
    "for article in articles:\n",
    "  headline = article.find(\"a\").text.strip()\n",
    "  headlines.append(headline)\n",
    "  time = article.find('time')\n",
    "  times.append(time)\n",
    "  link = article.find(\"a\")[\"href\"]\n",
    "  links.append(link)\n",
    "    \n",
    "data = {\n",
    "  \"Headline\": headlines,\n",
    "  \"Time\": times,\n",
    "  \"News Link\": links\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "166ee4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all the header tags :\n",
      "\n",
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\"><span class=\"mw-page-title-main\">Main Page</span></h1>\n",
      "\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfl-h2\"><span id=\"From_today.27s_featured_list\"></span><span class=\"mw-headline\" id=\"From_today's_featured_list\">From today's featured list</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n"
     ]
    }
   ],
   "source": [
    "#Ans1:#Ans1:\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "print('List all the header tags :', *titles, sep='\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "846b5e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Name\n",
      "0         Shri Ram Nath Kovind14th President of India\n",
      "1        Shri Pranab Mukherjee13th President of India\n",
      "2    Smt Pratibha Devisingh Patil12th President of...\n",
      "3       DR. A.P.J. Abdul Kalam11th President of India\n",
      "4         Shri K. R. Narayanan10th President of India\n",
      "5      Dr Shankar Dayal Sharma9th  President of India\n",
      "6           Shri R Venkataraman8th President of India\n",
      "7              Giani Zail Singh7th President of India\n",
      "8     Shri Neelam Sanjiva Reddy6th President of India\n",
      "9      Dr. Fakhruddin Ali Ahmed5th President of India\n",
      "10   Shri Varahagiri Venkata Giri4th President of ...\n",
      "11             Dr. Zakir Husain3rd President of India\n",
      "12   Dr. Sarvepalli Radhakrishnan2nd President of ...\n",
      "13          Dr. Rajendra Prasad1st President of India\n"
     ]
    }
   ],
   "source": [
    "#Ans2:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_former_presidents(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        presidents_list = []\n",
    "        \n",
    "        for president_card in soup.find_all('div', class_= \"president-listing\"):\n",
    "            name = president_card.text.replace(\"\\n\",\"\")\n",
    "            \n",
    "            president_data = {\n",
    "                'Name': name,\n",
    "            }\n",
    "            \n",
    "            presidents_list.append(president_data)\n",
    "        df = pd.DataFrame(presidents_list)\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to fetch data from {url}\")\n",
    "        return None\n",
    "if __name__ == \"__main__\":\n",
    "    presidents_url = 'https://presidentofindia.nic.in/former-presidents'\n",
    "    df = scrape_former_presidents(presidents_url)\n",
    "    if df is not None:\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc2caff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Ans6:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "articles_container = soup.find(\"div\", class_=\"sc-orwwe2-3 jOMrrY\")\n",
    "data =[]\n",
    "for article in articles_container.find_all(\"div\", class_=\"sc-orwwe2-3 jOMrrY\"):\n",
    "  title = article.find(\"div\", class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\").text.strip()\n",
    "  titles.append(title)\n",
    "  author = article.find(\"div\", class_=\"sc-1w3fpd7-0 dnCnAO\").text.strip()\n",
    "  authors.append(author)\n",
    "  date = article.find(\"div\", class_=\"sc-1thf9ly-2 dvggWt\").text.strip()\n",
    "  dates.append(date)\n",
    "  url = article.find(\"a\")[\"href\"]\n",
    "  urls.append(url)\n",
    "  data.apppend([title,author,date,url]) \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21c1fbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Ans7:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.dineout.co.in\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "restaurant_names = soup.find_all('h2', class_='restnt-name ellipsis')\n",
    "cuisines = soup.find_all('span', class_='double-line-ellipsis')\n",
    "locations = soup.find_all('span', class_='double-line-ellipsis')\n",
    "ratings = soup.find_all('span', class_='rating-value')\n",
    "image_urls = soup.find_all('img', class_='img-responsive')\n",
    "\n",
    "restaurant_list = []\n",
    "cuisine_list = []\n",
    "location_list = []\n",
    "rating_list = []\n",
    "image_url_list = []\n",
    "for name in restaurant_names:\n",
    "  restaurant_list.append(name.text.strip())\n",
    "for cuisine in cuisines:\n",
    "  cuisine_list.append(cuisine.text.strip())\n",
    "for location in locations:\n",
    "  location_list.append(location.text.strip())\n",
    "for rating in ratings:\n",
    "  rating_list.append(rating.text.strip())\n",
    "for image in image_urls:\n",
    "  image_url_list.append(image['src'])\n",
    "data = {\n",
    "  'Restaurant Name': restaurant_list,\n",
    "  'Cuisine': cuisine_list,\n",
    "  'Location': location_list,\n",
    "  'Ratings': rating_list,\n",
    "  'Image URL': image_url_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc7c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63fb12e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798c1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f402dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98430530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
